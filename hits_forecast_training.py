# -*- coding: utf-8 -*-
"""hits_forecast_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tueUPDiYnpNfwmsqObEFdFmRwbKXX-VF
"""

!pip install tqdm
!pip install keras
!rm -f drive/app/NASA*
!wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz -P drive/app
!gunzip drive/app/NASA_access_log_Jul95.gz
!wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz -P drive/app
!gunzip drive/app/NASA_access_log_Aug95.gz
!mv drive/app/NASA_access_log_Jul95 drive/app/NASA.log
!cat drive/app/NASA_access_log_Aug95 >> drive/app/NASA.log
!head drive/app/NASA.log
!wc -l drive/app/NASA.log

import string
import datetime

import numpy as np
import matplotlib.pyplot as plt

from time import time
from tqdm import tqdm
from functools import reduce
from collections import Counter

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from google.colab import files


from sklearn.preprocessing import MinMaxScaler

# Parameters
DATASET = "drive/app/NASA.log"
WINDOW_SIZE = 3
NB_HIDDEN_LAYERS = 100

def load_dataset():
    dataset = []
    with open(DATASET, 'r', encoding='utf-8', errors='ignore') as f:
      for line in tqdm(f):
        dataset.append("".join(l for l in line.replace("\n", "") if l in string.printable))

    print(len(dataset))    
    return dataset

def parse_line(line):
    d = datetime.datetime.strptime(line.split(' ')[3][1:], "%d/%b/%Y:%H:%M:%S")
    bytes_in_reply = line.split(' ')[-1]
    bytes_in_reply = 0 if bytes_in_reply == '-' else int(bytes_in_reply)
          
    return (d.toordinal(), d.time().hour, d.weekday(), d.month, bytes_in_reply)

def parse_dataset(dataset):
    parsed_lines = []
    for i, line in tqdm(enumerate(dataset)):
        parsed_lines.append(parse_line(line))

    return parsed_lines

def build_metadata(max_values, min_values):
    metadata = {'window_size': WINDOW_SIZE, 'min_values': min_values, 'max_values': max_values}

    with open(args.output + '.metadata.bin', 'wb') as f:
        pk.dump(metadata, f)

# This function is not written in a pythonic way to better handle massive amount of data
def build_data(parsed_dataset):
    data = []
    count = 0
    data_amount = 0
    previous_sample = parsed_dataset[0]
    for i in tqdm(range(1, len(parsed_dataset))):
        if parsed_dataset[i][:3] == previous_sample[:3]:
            data_amount += previous_sample[4]
            count += 1
        else:
            data.append(np.array((count, previous_sample[1], data_amount), dtype=np.float32))
            previous_sample = parsed_dataset[i]
            data_amount = previous_sample[4]
            count = 1
      
    # Scale all features btw 0 and 1
    scale = MinMaxScaler(feature_range=(0, 1))
    n_data = scale.fit_transform(data)
    data_max = scale.data_max_
    data_min = scale.data_min_
    
    X = []
    for i in range(0, len(n_data) - WINDOW_SIZE):
        X.append(np.array(n_data[i:i+WINDOW_SIZE]))
    
    y = []
    for i in range(0, len(n_data) - WINDOW_SIZE):
        y.append(n_data[i+WINDOW_SIZE])
    
      
    return X, y, data, scale

# Here, either we take the first ratio*100 percent or the last to get the testing data (because ordering matters!)
def split_data(X, y, ratio=0.20):
    after = np.random.randint(0,2)
    cut = round(len(X) - ratio * len(X)) if after else round(ratio * len(X)) 
    
    X_test = np.array(X[cut:]) if after else np.array(X[:cut])
    y_test = np.array(y[cut:])[:, :1] if after else np.array(y[:cut])[:, :1]
      
    X_train = np.array(X[:cut]) if after else np.array(X[cut:])
    y_train = np.array(y[:cut])[:, :1] if after else np.array(y[cut:])[:, :1]
    
    print("Number of training samples: {}".format(len(X_train)))
    print("Number of testing samples: {}".format(len(X_test)))
    
    return X_train, y_train, X_test, y_test

def display_info(data):
  hours = np.arange(24)
  x_hours = hours
  for i in range(1, round(len(data) / 24)):
    x_hours = np.concatenate([x_hours, hours])
  
  y_sums = np.array([x[0] for x in data[:24]]) 
  for i in range(24, len(data), 24):
    y_sums += np.array([x[0] for x in data[i:i+24]])
  
  y_means = y_sums / (len(data) / 24)
  
  y_stds = np.array([np.power(y[0] - y_means[i], 2) for i, y in enumerate(data[:24])])
  for i in range(24, len(data), 24):
    y_stds += np.array([np.power(y[0] - y_means[j % 24], 2) for j, y in enumerate(data[i:i+24])])
  
  y_stds = np.sqrt(y_stds / y_sums)
   
  y_stds = y_stds.astype(int)
  y_means = y_means.astype(int)  
    
  fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(50, 10))
  ax1.scatter(x=x_hours, y=[x[0] for x in data], marker='o', c='r', edgecolor='b', s=2)
  ax1.set_xlabel('Hour', fontsize=20)
  ax1.set_ylabel('Request hits number', fontsize=20)
 
  ax2.errorbar(np.arange(24), y_means, yerr=2*y_stds, fmt='o', ecolor='r', capthick=2)
  #ax2.scatter(x=x_hours, y=[x[0] for x in data], marker='o', c='r', edgecolor='b', s=2)
  ax2.set_xlabel('Hour', fontsize=20)
  ax2.set_ylabel('Mean and standard deviation error', fontsize=20)
  
  ax3.plot([x[0] for x in data], c='r')
  ax3.set_xlabel('Hour', fontsize=20)
  ax3.set_ylabel('Request hits number', fontsize=20)
                                 
  ax4.plot([x[2] for x in data], c='b')
  ax4.set_xlabel('Hour', fontsize=20)
  ax4.set_ylabel('Data amount', fontsize=20)

def train(X_train, y_train, X_test, Y_test):
    model = Sequential()
    model.add(LSTM(NB_HIDDEN_LAYERS, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(Dense(1))
    model.compile(loss='mae', optimizer='adam')
    model.fit(X_train, y_train, epochs=500, batch_size=32, validation_data=(X_test, y_test), verbose=2, shuffle=False)
    
    return model

def compare_plot(model, scale, X_test, y_test):
  results = model.predict(X_test, batch_size=32) 

  min_scale = scale.data_max_[0]
  max_scale = scale.data_max_[1]
  
  y_test_for_plot = y_test * (max_scale - min_scale) + min_scale
  results_for_plot = results * (max_scale - min_scale) + min_scale
  
  y_diff = y_test_for_plot - results_for_plot
  
  y_diff_mean = np.mean(y_diff)
  y_diff_std = np.std(y_diff)
  
  print("Mean: {}, Std: {}".format(y_diff_mean, y_diff_std))
    
  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25, 10))
  ax1.plot([y[0] for y in y_test_for_plot], c='g')
  ax1.plot([r[0] for r in results_for_plot], c='b')
  ax1.set_xlabel('Hour', fontsize=20)
  ax1.set_ylabel('Request hits number (green = ground truth)', fontsize=20)
 
  ax2.plot([y[0] for y in y_diff], c='r')
  ax2.set_ylim([-2000, 15000])
  ax2.set_xlabel('Hour', fontsize=20)
  ax2.set_ylabel('Difference ground truth minus prediction', fontsize=20)
  
  plt.show()

dataset = load_dataset()

start = time()
parsed_dataset = parse_dataset(dataset)
print("{} seconds spent on parsing data".format(time() - start))

start = time()
X, y, data, scale = build_data(parsed_dataset)
print("{} seconds spent on building data".format(time() - start))

start = time()
X_train, y_train, X_test, y_test = split_data(X, y)
print("{} seconds spent on splitting data".format(time() - start))

display_info(data)

model = train(X_train, y_train, X_test, y_test)

compare_plot(model, scale, X_test, y_test)

